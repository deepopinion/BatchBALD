* General process
* Infos
- ~logits_B_K_C~
  - B :: The batch put into the system
  - K :: The amount of MC samples drawn for each part of the batch
  - C :: Number of classes (one neuron per class obviously)
* Classes
** BayesianModule
#+BEGIN_SRC python
  # Returns B x n x output
  def forward(self, input_B: torch.Tensor, k: int):
      BayesianModule.k = k

      # First do the deterministic part of the network that won't change for the k samples
      input_B = self.deterministic_forward_impl(input_B)
      # Blow up output of deterministic forward part to be able to process k samples at the same time
      mc_input_BK = BayesianModule.mc_tensor(input_B, k)
      # Send the k deterministic inputs through the non-deterministic part
      mc_output_BK = self.mc_forward_impl(mc_input_BK)
      # Bring tensor back to correct output
      mc_output_B_K = BayesianModule.unflatten_tensor(mc_output_BK, k)
      return mc_output_B_K
#+END_SRC
